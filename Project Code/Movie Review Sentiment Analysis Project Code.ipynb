{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWMMm_Hr0E1K"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "267HfR5lfqn2"
      },
      "source": [
        "## Importing Libraries and Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yKCFv9MQMnU7"
      },
      "outputs": [],
      "source": [
        "import numpy as np # For working with arrays\n",
        "import pandas as pd # For working with datasets\n",
        "from pandas.plotting import scatter_matrix\n",
        "import matplotlib.pyplot as plt # For data visualization\n",
        "import seaborn as sns # For statistical graphing\n",
        "import re # For regular expressions\n",
        "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer # Determines if text is a neutral, negative, or positive sentiment\n",
        "from sklearn.model_selection import train_test_split # For splitting the data for regression analysis\n",
        "from sklearn.preprocessing import StandardScaler # For scaling the data for regression analysis\n",
        "from sklearn.linear_model import LinearRegression # Regression Analysis\n",
        "from sklearn.metrics import mean_squared_error, r2_score # Further Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TH6qJKIxO7ON",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "4de5bb19-4355-40fe-b7a7-f0fbe7d4c23d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'boxoffice2014_2023.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1686800622.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mboxoffice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'boxoffice2014_2023.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtomato_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rotten_tomatoes_critic_reviews.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtomato_movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rotten_tomatoes_movies.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimdb_movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'IMDB_Movies.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'boxoffice2014_2023.csv'"
          ]
        }
      ],
      "source": [
        "boxoffice = pd.read_csv('boxoffice2014_2023.csv')\n",
        "tomato_critic = pd.read_csv('rotten_tomatoes_critic_reviews.csv')\n",
        "tomato_movies = pd.read_csv('rotten_tomatoes_movies.csv')\n",
        "imdb_movies = pd.read_csv('IMDB_Movies.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hoeBsW0zrX2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MwYCg-JQqI_"
      },
      "outputs": [],
      "source": [
        "boxoffice.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSihqHt9QyOz"
      },
      "outputs": [],
      "source": [
        "tomato_critic.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu-iySBCQ1o0"
      },
      "outputs": [],
      "source": [
        "tomato_movies.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9-puCPXQ5Ze"
      },
      "outputs": [],
      "source": [
        "imdb_movies.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K637vhjQHRN"
      },
      "source": [
        "## Data Cleaning and Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TgJsxdLSWTp"
      },
      "source": [
        "### Checking for Data Anomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pp3betuRitB"
      },
      "source": [
        "#### Box Office Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYTzHrRDd8pO"
      },
      "outputs": [],
      "source": [
        "boxoffice.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_85ihlHd-Q8"
      },
      "outputs": [],
      "source": [
        "boxoffice.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZKBwsC-Rmtx"
      },
      "source": [
        "#### Rotten Tomato Critic Review Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4ubDaspR6E1"
      },
      "outputs": [],
      "source": [
        "tomato_critic.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkxpPx78R6fA"
      },
      "outputs": [],
      "source": [
        "tomato_critic.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ex2uzRsRt24"
      },
      "source": [
        "#### Rotten Tomato Movies Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovkfYr_qR7kc"
      },
      "outputs": [],
      "source": [
        "tomato_movies.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8N9Wl_eR7ao"
      },
      "outputs": [],
      "source": [
        "tomato_movies.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh41jyvWR8lr"
      },
      "source": [
        "#### IMDB Movies Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMgbXNP4Spzu"
      },
      "outputs": [],
      "source": [
        "imdb_movies.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZhhyF2NSr1X"
      },
      "outputs": [],
      "source": [
        "imdb_movies.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVLU2ZgMS78Z"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkPuPOmbTFCf"
      },
      "source": [
        "#### Finding and Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvG4qK_cXE75"
      },
      "outputs": [],
      "source": [
        "def FindingNulls(data):\n",
        "  print(\"Count of Initial Missing Values\")\n",
        "  print(data.isnull().sum())\n",
        "  missing_rows = data[data.isnull().any(axis = 1)]\n",
        "  print(\"\\nRows with missing values:\\n\")\n",
        "  print(missing_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c40bHxVpabkS"
      },
      "outputs": [],
      "source": [
        "# Removing Categorical Null values and Interpolating(?) numerical null values\n",
        "def HandlingNulls(data, interpolate_method = 'linear'):\n",
        "  categorical_col = data.select_dtypes(include = ['object', 'category']).columns\n",
        "  numerical_col = data.select_dtypes(include = [np.number]).columns\n",
        "\n",
        "  nonull_data = data.dropna(subset = categorical_col)\n",
        "\n",
        "  for column in numerical_col:\n",
        "    if nonull_data[column].isnull().any():\n",
        "      nonull_data[column] = nonull_data[column].interpolate(method = interpolate_method)\n",
        "  nonull_data[numerical_col] = nonull_data[numerical_col].fillna(method = 'ffill').fillna(method = 'bfill')\n",
        "\n",
        "  return nonull_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHGiZN51ZSty"
      },
      "source": [
        "###### Box Office Nulls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVN1T5Q_ZA9G"
      },
      "outputs": [],
      "source": [
        "FindingNulls(boxoffice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20-2UmGJZHX8"
      },
      "outputs": [],
      "source": [
        "boxoffice = HandlingNulls(boxoffice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkpZSfMuZwfv"
      },
      "outputs": [],
      "source": [
        "boxoffice.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olEyawsbe4E5"
      },
      "outputs": [],
      "source": [
        "boxoffice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onypg1j3Yy7U"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9G724QwfA9d"
      },
      "source": [
        "##### Tomato Critic Review Nulls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_im5fLvOfJgC"
      },
      "outputs": [],
      "source": [
        "FindingNulls(tomato_critic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI40syagfNyx"
      },
      "outputs": [],
      "source": [
        "tomato_critic = HandlingNulls(tomato_critic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZo-Q4LSfXW1"
      },
      "outputs": [],
      "source": [
        "tomato_critic.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzB2r0JgfaMT"
      },
      "outputs": [],
      "source": [
        "tomato_critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFKGcnZbfkgb"
      },
      "source": [
        "##### Tomato Movie Reviews Nulls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujbsRirkfqnm"
      },
      "outputs": [],
      "source": [
        "FindingNulls(tomato_movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHiO1pO9ft6t"
      },
      "outputs": [],
      "source": [
        "tomato_movies = HandlingNulls(tomato_movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y5JaGukfy7F"
      },
      "outputs": [],
      "source": [
        "tomato_movies.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj5w8zc-f1UF"
      },
      "outputs": [],
      "source": [
        "tomato_movies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn7BP2T-fmwx"
      },
      "source": [
        "##### IMDB Movies Nulls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-r7yEP6frJd"
      },
      "outputs": [],
      "source": [
        "FindingNulls(imdb_movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJuWsML0gCr9"
      },
      "outputs": [],
      "source": [
        "imdb_movies = HandlingNulls(imdb_movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pikW-uJgFY5"
      },
      "outputs": [],
      "source": [
        "imdb_movies.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O72shHm9gKP0"
      },
      "outputs": [],
      "source": [
        "imdb_movies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xw9FKclTC7d"
      },
      "source": [
        "#### Finding and Removing Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdQ50yJlTCWY"
      },
      "outputs": [],
      "source": [
        "def FindDuplicates(data):\n",
        "  data_duplicates = data[data.duplicated(keep = False)]\n",
        "  data_duplicates.to_csv(index = False)\n",
        "  print(\"Duplicate Rows:\\n\")\n",
        "  print(data_duplicates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLVodVHxgVHR"
      },
      "outputs": [],
      "source": [
        "def RemoveDuplicates(data, subset = None, keep = 'first'):\n",
        "  duplicate_count = data.duplicated(subset = subset, keep = keep).sum()\n",
        "  data_cleaned = data.drop_duplicates(subset = subset, keep = keep)\n",
        "  removed_duplicates = len(data) - len(data_cleaned)\n",
        "  print(\"Removed the duplicates from the dataset\", removed_duplicates)\n",
        "  data = data_cleaned\n",
        "  return data_cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AePce7VurDsD"
      },
      "source": [
        "##### Box Office Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL5XHQfRVaDG"
      },
      "outputs": [],
      "source": [
        "FindDuplicates(boxoffice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_UHwqTQTI4H"
      },
      "outputs": [],
      "source": [
        "boxoffice = RemoveDuplicates(boxoffice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx2E4bugq4hJ"
      },
      "outputs": [],
      "source": [
        "FindDuplicates(boxoffice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1ODrt6uq-KY"
      },
      "outputs": [],
      "source": [
        "boxoffice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCcTRRAyrK88"
      },
      "source": [
        "##### Tomato Critic Reviews Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7iIR7fnrOcq"
      },
      "outputs": [],
      "source": [
        "FindDuplicates(tomato_critic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBySwKx7rYBN"
      },
      "outputs": [],
      "source": [
        "tomato_critic = RemoveDuplicates(tomato_critic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivm1WyFcrYUZ"
      },
      "outputs": [],
      "source": [
        "FindDuplicates(tomato_critic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLMRhI37rYjB"
      },
      "outputs": [],
      "source": [
        "tomato_critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzjGE8v8rO5A"
      },
      "source": [
        "###### Tomato Movies Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFAvFAEqrTrx"
      },
      "outputs": [],
      "source": [
        "FindDuplicates(tomato_movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYbYikZrrhyY"
      },
      "outputs": [],
      "source": [
        "tomato_movies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTN1DBTIrUFT"
      },
      "source": [
        "##### IMDB Movies Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBSnFr3crsNU"
      },
      "outputs": [],
      "source": [
        "FindDuplicates(imdb_movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5DGzYTprsm7"
      },
      "outputs": [],
      "source": [
        "imdb_movies = RemoveDuplicates(imdb_movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCFrj_t5rs5x"
      },
      "outputs": [],
      "source": [
        "FindDuplicates(imdb_movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64kK7D9crtJw"
      },
      "outputs": [],
      "source": [
        "imdb_movies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRSGTvQiTJKZ"
      },
      "source": [
        "#### Cleaning Textual Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKlWN6agTOJW"
      },
      "outputs": [],
      "source": [
        "def CleanTextualData(data):\n",
        "  text_cols = data.select_dtypes(include = ['object', 'string']).columns\n",
        "  def CleanText(text):\n",
        "    if isinstance(text, str):\n",
        "      text = text.lower()\n",
        "      text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "      text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "  for column in text_cols:\n",
        "    data[column] = data[column].apply(CleanText)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmX18Wqtvmmd"
      },
      "source": [
        "##### Box Office Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN6qga2cv5VW"
      },
      "outputs": [],
      "source": [
        "boxoffice_cleaned = CleanTextualData(boxoffice)\n",
        "boxoffice_cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vt64AEbvpiZ"
      },
      "source": [
        "##### Tomato Critic Reviews Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spodaHKavicg"
      },
      "outputs": [],
      "source": [
        "tomato_critic_cleaned = CleanTextualData(tomato_critic)\n",
        "tomato_critic_cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2TB6Yauvqkp"
      },
      "source": [
        "##### Tomato Movies Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e70FALxjv7Sk"
      },
      "outputs": [],
      "source": [
        "tomato_movies_cleaned = CleanTextualData(tomato_movies)\n",
        "tomato_movies_cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN5aTtnYvrWe"
      },
      "source": [
        "##### IMDB Movies Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELKGiOEqvqBz"
      },
      "outputs": [],
      "source": [
        "imdb_movies_cleaned = CleanTextualData(imdb_movies)\n",
        "imdb_movies_cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8bm7Ommz20A"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QkGqeaKZTj1"
      },
      "source": [
        "### Finding Correlated Features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.downloader.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "Z4i6OTUYq518"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "positive_words = ['good', 'great', 'excellent', 'awesome', 'amazing', 'love', 'fantastic', 'positive']\n",
        "negative_words = ['bad', 'poor', 'terrible', 'awful', 'hate', 'negative', 'worst', 'disappointing']"
      ],
      "metadata": {
        "id": "Q3hOuWHCrNmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SentimentAnalysis(text):\n",
        "  if not isinstance(text, str):\n",
        "    return 0 # Neutral sentiment for non-string inputs\n",
        "  text = text.lower()\n",
        "  pos_count = sum(word in text for word in positive_words)\n",
        "  neg_count = sum(word in text for word in negative_words)\n",
        "  return pos_count - neg_count\n",
        "\n",
        "def AddTextFeatures(df, text_col):\n",
        "  if text_col in df.columns:\n",
        "    df['sentiment'] = df[text_col].apply(SentimentAnalysis)\n",
        "    df['text_length'] = df[text_col].apply(lambda x: len(str(x)))\n",
        "    df['word_count'] = df[text_col].apply(lambda x: len(str(x).split()))\n",
        "    return df\n",
        "  else:\n",
        "    print(f\"Column '{text_col}' not found in the DataFrame.\")\n",
        "    return df\n",
        "\n",
        "def AddTfidfFeatures(df, text_col, max_features = 12):\n",
        "  if text_col in df.columns:\n",
        "    vectorizer = TfidfVectorizer(max_features = max_features)\n",
        "    tfidf_matrix = vectorizer.fit_transform(df[text_col].astype(str))\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns = [f'tfidf_{i}' for i in range(max_features)])\n",
        "    df = pd.concat([df.reset_index(drop = True), tfidf_df.reset_index(drop = True)], axis = 1)\n",
        "  return df\n",
        "\n",
        "def EncodeCategorical(df):\n",
        "  label_enc = LabelEncoder()\n",
        "  for col in df.select_dtypes(include = ['object']).columns:\n",
        "    df[col] = label_enc.fit_transform(df[col])\n",
        "  return df\n",
        "\n",
        "def AddDateFeatures(df, date_col):\n",
        "  if date_col in df.columns:\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors = \"coerce\")\n",
        "    df['year'] = df[date_col].dt.year\n",
        "    df['month'] = df[date_col].dt.month\n",
        "    df['day'] = df[date_col].dt.day\n",
        "    df.drop(columns = [date_col], inplace = True)\n",
        "  return df\n",
        "\n",
        "def StandardizeNumerical(df):\n",
        "  scaler = StandardScaler()\n",
        "  numerical_cols = df.select_dtypes(include = [np.number]).columns\n",
        "  df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "  return df"
      ],
      "metadata": {
        "id": "bbor5gO1rO0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ProcessedDataset(df, text_col = None, date_col = None, max_tfidf_features = 12):\n",
        "  if text_col:\n",
        "    df = AddTextFeatures(df, text_col)\n",
        "    df = AddTfidfFeatures(df, text_col, max_features = max_tfidf_features)\n",
        "    df = EncodeCategorical(df)\n",
        "  if date_col:\n",
        "    df = AddDateFeatures(df, date_col)\n",
        "  df = StandardizeNumerical(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "x732LYh-w4Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tomato_critic_feat_eng = ProcessedDataset(tomato_critic_cleaned, text_col = 'review_content', date_col = 'review_date')\n",
        "tomato_movies_feat_eng = ProcessedDataset(tomato_movies_cleaned, text_col = 'critic_consensus', date_col = 'release_date')\n",
        "imdb_movies_feat_eng = ProcessedDataset(imdb_movies_cleaned, text_col = 'review_text', date_col = 'release_date')"
      ],
      "metadata": {
        "id": "Ye7xN1Yj5ALl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tomato_critic_feat_eng"
      ],
      "metadata": {
        "id": "tJmXhfZF5rd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tomato_movies_feat_eng"
      ],
      "metadata": {
        "id": "G86sDup8_cv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_movies_feat_eng"
      ],
      "metadata": {
        "id": "DPPNwJsE_fVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CorrelationMatrix(data, data_name = \"Dataset\", font_size = 10):\n",
        "  corr_matrix = data.corr()\n",
        "  mask = np.triu(np.ones_like(corr_matrix, dtype = bool))\n",
        "\n",
        "  print(f\"Correlation Matrix for {data_name}\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  plt.figure(figsize = (12, 10))\n",
        "  sns.heatmap(corr_matrix, mask = mask, annot = True, cmap = 'coolwarm', fmt = '.2f', cbar = True, annot_kws = {\"size\": font_size})\n",
        "  plt.title(f\"Correlation Matrix Heatmap: {data_name}\", fontsize = font_size + 4)\n",
        "  plt.xticks(fontsize = font_size)\n",
        "  plt.yticks(fontsize = font_size)\n",
        "  plt.show()\n",
        "\n",
        "  return corr_matrix\n"
      ],
      "metadata": {
        "id": "vLfka1CA9yIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CorrelationMatrix(tomato_critic_feat_eng, \"Tomato Critic Reviews\")"
      ],
      "metadata": {
        "id": "ujgvq9cM_7mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CorrelationMatrix(tomato_movies_feat_eng, \"Tomato Movies\")"
      ],
      "metadata": {
        "id": "haZiK0A6AQC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CorrelationMatrix(imdb_movies_feat_eng, \"IMDB Movies\", font_size = 8)"
      ],
      "metadata": {
        "id": "YzrEPAT-AR2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wj8P4qefkdv"
      },
      "source": [
        "# Model Building\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentiment Analysis**\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "YsSFQp59HOvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Hugging Face Transformers\n",
        "!pip install transformers\n",
        "\n",
        "# Import pipeline for sentiment analysis\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the emotion classification model\n",
        "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n",
        "\n",
        "# Test the model\n",
        "print(classifier(\"This movie is fantastic!\"))\n"
      ],
      "metadata": {
        "id": "yvvqxvmVHV2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to classify emotions in a dataset\n",
        "def classify_reviews(data, text_column):\n",
        "    # Apply the classifier to the text column\n",
        "    data['emotion_scores'] = data[text_column].apply(classifier)\n",
        "    return data\n",
        "\n",
        "# Apply to the critic reviews dataset (adjust 'review_text' to match your dataset's column name)\n",
        "# This will generate emotion scores for each review\n",
        "tomato_critic_with_emotions = classify_reviews(tomato_critic_cleaned, 'review')\n",
        "\n",
        "# Display the dataset with the new emotion scores\n",
        "tomato_critic_with_emotions.head()\n"
      ],
      "metadata": {
        "id": "D02f5qEuHcNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison Plots"
      ],
      "metadata": {
        "id": "Gd0KE8L4ykI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def year(x):\n",
        "  return x[:4]\n",
        "\n",
        "def boxPlotByYear(data, name = ''):\n",
        "  fig, ax = plt.subplots(figsize =(15, 4))\n",
        "  bp = ax.boxplot(data)\n",
        "  ax.set_xticklabels(range(2006,2017))\n",
        "  plt.title(name)\n",
        "  plt.show()\n",
        "\n",
        "runtimeDataR = []\n",
        "audienceCountDataR = []\n",
        "expertCountDataR = []\n",
        "scoreDataR = []\n",
        "runtimeDataI = []\n",
        "audienceCountDataI = []\n",
        "expertCountDataI = []\n",
        "scoreDataI = []\n",
        "\n",
        "for curYear in range(2006,2017):\n",
        "  tomato_box = tomato_movies.loc[tomato_movies['original_release_date'].apply(year) == str(curYear), ['movie_title', 'original_release_date', 'runtime', 'tomatometer_rating', 'audience_rating','tomatometer_count','audience_count']]\n",
        "  imbd_box = imdb_movies.loc[imdb_movies['title_year'] == curYear, ['movie_title', 'title_year', 'duration', 'num_critic_for_reviews','num_voted_users','imdb_score']]\n",
        "  tomato_box['overall_score'] = (tomato_box['tomatometer_rating'] + tomato_box['audience_rating']) * 9 / 200 + 1\n",
        "  tomato_box.drop(columns = ['tomatometer_rating', 'audience_rating'], inplace = True)\n",
        "  runtimeDataR.append(tomato_box['runtime'])\n",
        "  audienceCountDataR.append(tomato_box['audience_count'])\n",
        "  expertCountDataR.append(tomato_box['tomatometer_count'])\n",
        "  scoreDataR.append(tomato_box['overall_score'])\n",
        "  runtimeDataI.append(imbd_box['duration'])\n",
        "  audienceCountDataI.append(imbd_box['num_voted_users'].apply(int))\n",
        "  expertCountDataI.append(imbd_box['num_critic_for_reviews'])\n",
        "  scoreDataI.append(imbd_box['imdb_score'])\n",
        "\n",
        "boxPlotByYear(runtimeDataR, \"Rotton Tomatoes Runtime Distribution\")\n",
        "boxPlotByYear(runtimeDataI, \"IMDB Runtime Distribution\")\n",
        "boxPlotByYear(audienceCountDataR, \"Number of audience reviews in Rotton Tomatoes\")\n",
        "boxPlotByYear(audienceCountDataI, \"Number of audience reviews in IMDB\")\n",
        "boxPlotByYear(expertCountDataR, \"Number of expert reviews in Rotton Tomatoes\")\n",
        "boxPlotByYear(expertCountDataI, \"Number of expert reviews in IMDB\")\n",
        "boxPlotByYear(scoreDataR, \"Rotton Tomatoes adjusted score\")\n",
        "boxPlotByYear(scoreDataI, \"IMDB Score\")\n"
      ],
      "metadata": {
        "id": "r3GbWUIgy2Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RTc49dNOLdox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment analysis comparison"
      ],
      "metadata": {
        "id": "Ngotb0jWQm46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "uD-tiBKlQzYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nrclex import NRCLex\n",
        "\n",
        "sentimentSets = []\n",
        "for CurYear in range(2006,2017):\n",
        "  TomatoCriticInit = tomato_critic.copy()\n",
        "  TomatoCriticInit['year'] = TomatoCriticInit['review_date'].apply(year).apply(int)\n",
        "  TomatoCriticInit = TomatoCriticInit.loc[TomatoCriticInit['year'] == CurYear]\n",
        "  #print(ElonInit.head())\n",
        "  #TomatoCriticInit['text'] = TomatoCriticInit['review_content'].str.replace(r\"^\\\"?b['\\\"]\\\"?\\s+|\\s+['\\\"]\\\"?$|^\\\"?b['\\\"]\\\"?|'$|^\\s+|\\s+$\", '', regex=True)\n",
        "  #TomatoCriticInit['text'] = TomatoCriticInit['text'].str.replace(r'([\\s\"\\'])@\\w+', '', regex=True)\n",
        "  #TomatoCriticInit['text'] = TomatoCriticInit['text'].str.replace(r'^@\\w+\\s', '', regex=True)\n",
        "  TomatoCriticInit['text'] = TomatoCriticInit['review_content'].str.replace(r'\\\\xe2\\\\x..\\\\x..\\s|\\s?\\\\xe2\\\\x..\\\\x..', '', regex=True)\n",
        "  TomatoCriticInit['text'] = TomatoCriticInit['text'].str.translate(str.maketrans('', '', string.punctuation))\n",
        "  TomatoCriticInit['text'] = TomatoCriticInit['text'].str.replace(r'\\s?http\\S+', '', regex=True)\n",
        "\n",
        "  TomatoCriticInit['texttokens'] = TomatoCriticInit['text'].apply(word_tokenize)\n",
        "  TomatoCriticInit['texttokens'] = TomatoCriticInit['texttokens'].apply(lambda x: [word.lower() for word in x])\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  TomatoCriticInit['texttokens'] = TomatoCriticInit['texttokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  TomatoCriticInit['texttokens'] = TomatoCriticInit['texttokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "  ps = PorterStemmer()\n",
        "  TomatoCriticInit['texttokens'] = TomatoCriticInit['texttokens'].apply(lambda x: [ps.stem(word) for word in x])\n",
        "\n",
        "  FinSentences = TomatoCriticInit['texttokens'].apply(lambda x: \" \".join(x))\n",
        "  FinText = \" \".join(FinSentences)\n",
        "\n",
        "  #Vectorizers for ngrams of 1, 2, and 3, as well as a TF-IDF vectorizer\n",
        "  vectorizer = CountVectorizer(analyzer='word',stop_words='english')\n",
        "  ElonWordsVector = vectorizer.fit_transform([FinText])\n",
        "  vectorizer2 = CountVectorizer(analyzer='word',ngram_range=(2,2),stop_words='english')\n",
        "  ElonWordsVector2 = vectorizer2.fit_transform([FinText])\n",
        "  vectorizer3 = CountVectorizer(analyzer='word',ngram_range=(3,3),stop_words='english')\n",
        "  ElonWordsVector3 = vectorizer2.fit_transform([FinText])\n",
        "  TVectorizer = TfidfVectorizer()\n",
        "  ElonWordsTVector = TVectorizer.fit_transform(FinSentences)\n",
        "  feature_names = vectorizer2.get_feature_names_out()\n",
        "  #Print data\n",
        "  frequencies = ElonWordsVector2.sum(axis=0).A1\n",
        "\n",
        "  # Create a DataFrame to store feature names and frequencies\n",
        "  df = pd.DataFrame({'feature': feature_names, 'frequency': frequencies})\n",
        "\n",
        "  # Sort the DataFrame by frequency in descending order\n",
        "  df = df.sort_values('frequency', ascending=False)\n",
        "  print(f\"{CurYear} Dictionary:\")\n",
        "  print(df.head(n=10))\n",
        "\n",
        "\n",
        "  #Word cloud\n",
        "  #wordcloud = WordCloud(width = 800, height = 800, background_color ='white', min_font_size = 10).generate(FinText)\n",
        "  #plt.figure(facecolor = None)\n",
        "  #plt.imshow(wordcloud)\n",
        "  #plt.axis(\"off\")\n",
        "  #plt.tight_layout(pad = 0)\n",
        "  #plt.title(f\"{CurYear}'s Wordcloud\",fontsize=20)\n",
        "  #analysis = []\n",
        "  #plt.show()\n",
        "  #Sentiment analysis\n",
        "  #for sentence in FinSentences:\n",
        "  #    analysis.append(TextBlob(sentence).sentiment.polarity * 9 / 2 + 5.5)\n",
        "  #TomatoCriticInit['sentiment'] = analysis\n",
        "  #sentimentSets.append(TomatoCriticInit['sentiment'])\n",
        "  #print(TextBlob(FinText).sentiment.polarity)\n",
        "  #sia = SentimentIntensityAnalyzer()\n",
        "  #TomatoCriticInit['sentiment2'] = TomatoCriticInit['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "  #print(sia.polarity_scores(FinText))\n",
        "  #TomatoCriticInit['sentiment3'] = FinSentences.apply(lambda x: NRCLex(x).affect_frequencies)\n",
        "  #print(NRCLex(FinText).affect_frequencies)\n",
        "  #print(TomatoCriticInit.head())\n",
        "#boxPlotByYear(sentimentSets, \"Rotton Tomatoes Sentiment Score Distribution\")"
      ],
      "metadata": {
        "id": "3aGKHgukTPB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "\n",
        "# Define genre-defining terms\n",
        "genre_terms = {\n",
        "    \"Action\": [\"fight\", \"explosion\", \"battle\"],\n",
        "    \"Comedy\": [\"funny\", \"laugh\", \"joke\"],\n",
        "    \"Drama\": [\"emotional\", \"character\", \"dialogue\"],\n",
        "    \"Horror\": [\"scary\", \"fear\", \"monster\"],\n",
        "    \"Sci-Fi\": [\"alien\", \"future\", \"space\"]\n",
        "}\n",
        "\n",
        "# Load the critic reviews dataset\n",
        "rt_critic_reviews = pd.read_csv('rotten_tomatoes_critic_reviews.csv')\n",
        "\n",
        "# Extract year from review_date and filter for years >= 2000\n",
        "rt_critic_reviews['year'] = pd.to_datetime(rt_critic_reviews['review_date'], errors='coerce').dt.year\n",
        "rt_critic_reviews = rt_critic_reviews[rt_critic_reviews['year'] >= 2000]\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning\n",
        "rt_critic_reviews['processed_text'] = rt_critic_reviews['review_content'].fillna('').apply(clean_text)\n",
        "\n",
        "# Aggregate frequencies by year for genre terms\n",
        "genre_frequencies = {genre: [] for genre in genre_terms}\n",
        "years = sorted(rt_critic_reviews['year'].dropna().unique())\n",
        "\n",
        "for year in years:\n",
        "    yearly_reviews = rt_critic_reviews[rt_critic_reviews['year'] == year]['processed_text']\n",
        "    word_counts = Counter(\" \".join(yearly_reviews).split())\n",
        "\n",
        "    for genre, terms in genre_terms.items():\n",
        "        genre_frequencies[genre].append(sum(word_counts.get(term, 0) for term in terms))\n",
        "\n",
        "# Convert to a DataFrame for visualization\n",
        "genre_trends_df = pd.DataFrame(genre_frequencies, index=years)\n",
        "genre_trends_df.index.name = 'Year'\n",
        "\n",
        "# Plot the genre frequency trends\n",
        "genre_trends_df.plot(figsize=(12, 6), marker='o')\n",
        "plt.title(\"Genre Frequency Trends (2000 and Later)\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend(title=\"Genres\", bbox_to_anchor=(1.5, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eC4Bpt-yKioD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hmC9RuBnYDq3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "267HfR5lfqn2",
        "4TgJsxdLSWTp",
        "1Pp3betuRitB",
        "wZKBwsC-Rmtx",
        "1ex2uzRsRt24",
        "Fh41jyvWR8lr",
        "rkPuPOmbTFCf",
        "1Xw9FKclTC7d",
        "Gd0KE8L4ykI8",
        "Ngotb0jWQm46"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}